{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QhanUmoHa0s",
        "outputId": "45f24bd7-16ab-46f2-eb29-6794e9cfe3d5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kJF4x5jHHi02"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleTemporalUNet(nn.Module):\n",
        "    def __init__(self, in_channels=9, out_channels=3, base_filters=32):\n",
        "        super(SimpleTemporalUNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "        # Encoder path\n",
        "        self.enc1 = conv_block(in_channels, base_filters)       # 9 -> 32\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc2 = conv_block(base_filters, base_filters*2)    # 32 -> 64\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc3 = conv_block(base_filters*2, base_filters*4)  # 64 -> 128\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc4 = conv_block(base_filters*4, base_filters*8)  # 128 -> 256\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = conv_block(base_filters*8, base_filters*16)  # 256 -> 512\n",
        "\n",
        "        # Decoder path\n",
        "        self.up4 = nn.ConvTranspose2d(base_filters*16, base_filters*8, kernel_size=2, stride=2)\n",
        "        self.dec4 = conv_block(base_filters*16, base_filters*8)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(base_filters*8, base_filters*4, kernel_size=2, stride=2)\n",
        "        self.dec3 = conv_block(base_filters*8, base_filters*4)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(base_filters*4, base_filters*2, kernel_size=2, stride=2)\n",
        "        self.dec2 = conv_block(base_filters*4, base_filters*2)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(base_filters*2, base_filters, kernel_size=2, stride=2)\n",
        "        self.dec1 = conv_block(base_filters*2, base_filters)\n",
        "\n",
        "        # Final conv (to RGB output)\n",
        "        self.final_conv = nn.Conv2d(base_filters, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)        # [B, 32, H, W]\n",
        "        p1 = self.pool1(e1)      # Downsample\n",
        "\n",
        "        e2 = self.enc2(p1)       # [B, 64, H/2, W/2]\n",
        "        p2 = self.pool2(e2)\n",
        "\n",
        "        e3 = self.enc3(p2)       # [B, 128, H/4, W/4]\n",
        "        p3 = self.pool3(e3)\n",
        "\n",
        "        e4 = self.enc4(p3)       # [B, 256, H/8, W/8]\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(F.max_pool2d(e4, 2))  # [B, 512, H/16, W/16]\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.up4(b)                               # Upsample\n",
        "        d4 = torch.cat([d4, e4], dim=1)               # Skip connection\n",
        "        d4 = self.dec4(d4)\n",
        "\n",
        "        d3 = self.up3(d4)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        out = self.final_conv(d1)\n",
        "        out = torch.sigmoid(out)  # Normalize output to [0,1]\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XJO2nV0fHkxT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class VideoTripletDataset(Dataset):\n",
        "    def __init__(self, blur_dir, gt_dir):\n",
        "        self.blur_dir = blur_dir\n",
        "        self.gt_dir = gt_dir\n",
        "        self.samples = []\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "        for scene in sorted(os.listdir(blur_dir)):\n",
        "            blur_scene_path = os.path.join(blur_dir, scene)\n",
        "            gt_scene_path = os.path.join(gt_dir, scene)\n",
        "\n",
        "            if not os.path.isdir(blur_scene_path) or not os.path.isdir(gt_scene_path):\n",
        "                continue\n",
        "\n",
        "            blur_images = sorted(glob.glob(os.path.join(blur_scene_path, \"*.png\")))\n",
        "\n",
        "            for i in range(1, len(blur_images) - 1):\n",
        "                center_name = os.path.basename(blur_images[i])\n",
        "                gt_path = os.path.join(gt_scene_path, center_name)\n",
        "\n",
        "                if os.path.exists(gt_path):\n",
        "                    triplet = [blur_images[i-1], blur_images[i], blur_images[i+1]]\n",
        "                    self.samples.append((triplet, gt_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _read_rgb_image(self, path):\n",
        "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            raise IOError(f\"Failed to load image: {path}\")\n",
        "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        blur_paths, gt_path = self.samples[idx]\n",
        "\n",
        "        # Read triplet + GT\n",
        "        blurs = [self._read_rgb_image(p) for p in blur_paths]\n",
        "        gt = self._read_rgb_image(gt_path)\n",
        "\n",
        "        # Minimal consistent center crop to align sizes\n",
        "        h = min([img.shape[0] for img in blurs] + [gt.shape[0]])\n",
        "        w = min([img.shape[1] for img in blurs] + [gt.shape[1]])\n",
        "        h -= h % 16  # Optional: ensure divisible by 16 for UNet\n",
        "        w -= w % 16\n",
        "\n",
        "        def crop_center(img, h, w):\n",
        "            ch, cw = img.shape[:2]\n",
        "            start_y = (ch - h) // 2\n",
        "            start_x = (cw - w) // 2\n",
        "            return img[start_y:start_y+h, start_x:start_x+w]\n",
        "\n",
        "        blurs = [crop_center(img, h, w) for img in blurs]\n",
        "        gt = crop_center(gt, h, w)\n",
        "\n",
        "        # Stack 3 frames into 9-channel input\n",
        "        blur_tensor = torch.cat([self.transform(img) for img in blurs], dim=0)  # Shape: [9, H, W]\n",
        "        gt_tensor = self.transform(gt)  # Shape: [3, H, W]\n",
        "\n",
        "        return blur_tensor.float(), gt_tensor.float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "2XA4PSvqHplm",
        "outputId": "fc57479c-63a7-43c3-819c-6315991f53ba"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# Load your model and weights\n",
        "model = SimpleTemporalUNet()  # replace with your model initialization\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model_checkpoints/best_model.pth', map_location='cpu'))  # adjust path\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def video_to_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Convert BGR to RGB\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def frames_to_video(frames, output_path, fps=30):\n",
        "    height, width, _ = frames[0].shape\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "    for f in frames:\n",
        "        # Convert RGB to BGR for saving\n",
        "        f_bgr = cv2.cvtColor(f, cv2.COLOR_RGB2BGR)\n",
        "        out.write(f_bgr)\n",
        "    out.release()\n",
        "\n",
        "def pad_frames(frames, pad=1):\n",
        "    # Replicate first and last frames for temporal padding\n",
        "    return [frames[0]]*pad + frames + [frames[-1]]*pad\n",
        "\n",
        "def prepare_input(frames, idx):\n",
        "    # Stack frames idx-1, idx, idx+1 along channel dimension\n",
        "    prev_frame = to_tensor(frames[idx-1])\n",
        "    curr_frame = to_tensor(frames[idx])\n",
        "    next_frame = to_tensor(frames[idx+1])\n",
        "    input_tensor = torch.cat([prev_frame, curr_frame, next_frame], dim=0)\n",
        "    return input_tensor.unsqueeze(0).to(device)  # add batch dimension\n",
        "\n",
        "def deblur_video(video_path, output_path):\n",
        "    frames = video_to_frames(video_path)\n",
        "    if len(frames) < 3:\n",
        "        raise ValueError(\"Video must have at least 3 frames for temporal input\")\n",
        "\n",
        "    fps = cv2.VideoCapture(video_path).get(cv2.CAP_PROP_FPS)\n",
        "    padded_frames = pad_frames(frames, pad=1)\n",
        "\n",
        "    restored_frames = []\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(1, len(padded_frames) - 1), desc=\"Deblurring frames\"):\n",
        "            input_tensor = prepare_input(padded_frames, i)\n",
        "            output = model(input_tensor)\n",
        "            output = output.squeeze(0).cpu().clamp(0, 1)  # (3, H, W)\n",
        "            output_np = output.permute(1, 2, 0).numpy()  # H, W, 3\n",
        "            output_img = (output_np * 255).astype(np.uint8)\n",
        "            restored_frames.append(output_img)\n",
        "\n",
        "    frames_to_video(restored_frames, output_path, fps=int(fps))\n",
        "    print(f\"Restored video saved to: {output_path}\")\n",
        "\n",
        "    # Display output video inline in Colab\n",
        "    mp4 = open(output_path, 'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(f\"\"\"\n",
        "    <video width=640 controls>\n",
        "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\")\n",
        "\n",
        "# --- Usage in Colab ---\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    print(f'Processing {fn} ...')\n",
        "    output_html = deblur_video(fn, 'restored_output.mp4')\n",
        "\n",
        "display(output_html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pxMA34dVOOBf",
        "outputId": "348d6a1f-f715-4ace-8b82-4b72e51137b3"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "restored=\"/content/restored_output.mp4\"\n",
        "files.download(restored)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
