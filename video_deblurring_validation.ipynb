{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr0i7UZkjRfB",
        "outputId": "1979a290-87fc-4e62-c816-16728cc010ed"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cem5N8-MnnVt",
        "outputId": "3229bcc3-421e-43f9-a244-7fd2fa964eb6"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "gopro_zip_path = '/content/drive/MyDrive/GoPro.zip'  # Update if path is different\n",
        "gopro_extract_path = '/content/gopro_dataset/'\n",
        "\n",
        "# Unzipping GoPro\n",
        "with zipfile.ZipFile(gopro_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(gopro_extract_path)\n",
        "\n",
        "print(\" GoPro dataset unzipped to:\", gopro_extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS-TDxAhnpIk",
        "outputId": "a99c0e57-55c5-42c2-d774-c27affd54690"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def split_train_val(data_dir, val_ratio=0.1, seed=42):\n",
        "    blur_dir = os.path.join(data_dir, 'train', 'blur')\n",
        "    gt_dir = os.path.join(data_dir, 'train', 'GT')\n",
        "    val_blur_dir = os.path.join(data_dir, 'val', 'blur')\n",
        "    val_gt_dir = os.path.join(data_dir, 'val', 'GT')\n",
        "\n",
        "    os.makedirs(val_blur_dir, exist_ok=True)\n",
        "    os.makedirs(val_gt_dir, exist_ok=True)\n",
        "\n",
        "    scenes = sorted(os.listdir(blur_dir))\n",
        "    random.seed(seed)\n",
        "    val_scenes = random.sample(scenes, int(len(scenes) * val_ratio))\n",
        "\n",
        "    for scene in val_scenes:\n",
        "        shutil.move(os.path.join(blur_dir, scene), os.path.join(val_blur_dir, scene))\n",
        "        shutil.move(os.path.join(gt_dir, scene), os.path.join(val_gt_dir, scene))\n",
        "\n",
        "    print(f\"Moved {len(val_scenes)} scenes to validation set.\")\n",
        "\n",
        "# Example usage\n",
        "split_train_val('/content/gopro_dataset/GoPro', val_ratio=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rP4tIdA3nsXh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleTemporalUNet(nn.Module):\n",
        "    def __init__(self, in_channels=9, out_channels=3, base_filters=32):\n",
        "        super(SimpleTemporalUNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "        # Encoder path\n",
        "        self.enc1 = conv_block(in_channels, base_filters)       # 9 -> 32\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc2 = conv_block(base_filters, base_filters*2)    # 32 -> 64\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc3 = conv_block(base_filters*2, base_filters*4)  # 64 -> 128\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc4 = conv_block(base_filters*4, base_filters*8)  # 128 -> 256\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = conv_block(base_filters*8, base_filters*16)  # 256 -> 512\n",
        "\n",
        "        # Decoder path\n",
        "        self.up4 = nn.ConvTranspose2d(base_filters*16, base_filters*8, kernel_size=2, stride=2)\n",
        "        self.dec4 = conv_block(base_filters*16, base_filters*8)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(base_filters*8, base_filters*4, kernel_size=2, stride=2)\n",
        "        self.dec3 = conv_block(base_filters*8, base_filters*4)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(base_filters*4, base_filters*2, kernel_size=2, stride=2)\n",
        "        self.dec2 = conv_block(base_filters*4, base_filters*2)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(base_filters*2, base_filters, kernel_size=2, stride=2)\n",
        "        self.dec1 = conv_block(base_filters*2, base_filters)\n",
        "\n",
        "        # Final conv (to RGB output)\n",
        "        self.final_conv = nn.Conv2d(base_filters, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)        # [B, 32, H, W]\n",
        "        p1 = self.pool1(e1)      # Downsample\n",
        "\n",
        "        e2 = self.enc2(p1)       # [B, 64, H/2, W/2]\n",
        "        p2 = self.pool2(e2)\n",
        "\n",
        "        e3 = self.enc3(p2)       # [B, 128, H/4, W/4]\n",
        "        p3 = self.pool3(e3)\n",
        "\n",
        "        e4 = self.enc4(p3)       # [B, 256, H/8, W/8]\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(F.max_pool2d(e4, 2))  # [B, 512, H/16, W/16]\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.up4(b)                               # Upsample\n",
        "        d4 = torch.cat([d4, e4], dim=1)               # Skip connection\n",
        "        d4 = self.dec4(d4)\n",
        "\n",
        "        d3 = self.up3(d4)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        out = self.final_conv(d1)\n",
        "        out = torch.sigmoid(out)  # Normalize output to [0,1]\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T7NZTUwUnxBX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class VideoTripletDataset(Dataset):\n",
        "    def __init__(self, blur_dir, gt_dir):\n",
        "        self.blur_dir = blur_dir\n",
        "        self.gt_dir = gt_dir\n",
        "        self.samples = []\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "        for scene in sorted(os.listdir(blur_dir)):\n",
        "            blur_scene_path = os.path.join(blur_dir, scene)\n",
        "            gt_scene_path = os.path.join(gt_dir, scene)\n",
        "\n",
        "            if not os.path.isdir(blur_scene_path) or not os.path.isdir(gt_scene_path):\n",
        "                continue\n",
        "\n",
        "            blur_images = sorted(glob.glob(os.path.join(blur_scene_path, \"*.png\")))\n",
        "\n",
        "            for i in range(1, len(blur_images) - 1):\n",
        "                center_name = os.path.basename(blur_images[i])\n",
        "                gt_path = os.path.join(gt_scene_path, center_name)\n",
        "\n",
        "                if os.path.exists(gt_path):\n",
        "                    triplet = [blur_images[i-1], blur_images[i], blur_images[i+1]]\n",
        "                    self.samples.append((triplet, gt_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _read_rgb_image(self, path):\n",
        "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            raise IOError(f\"Failed to load image: {path}\")\n",
        "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        blur_paths, gt_path = self.samples[idx]\n",
        "\n",
        "        # Read triplet + GT\n",
        "        blurs = [self._read_rgb_image(p) for p in blur_paths]\n",
        "        gt = self._read_rgb_image(gt_path)\n",
        "\n",
        "        # Minimal consistent center crop to align sizes\n",
        "        h = min([img.shape[0] for img in blurs] + [gt.shape[0]])\n",
        "        w = min([img.shape[1] for img in blurs] + [gt.shape[1]])\n",
        "        h -= h % 16  # Optional: ensure divisible by 16 for UNet\n",
        "        w -= w % 16\n",
        "\n",
        "        def crop_center(img, h, w):\n",
        "            ch, cw = img.shape[:2]\n",
        "            start_y = (ch - h) // 2\n",
        "            start_x = (cw - w) // 2\n",
        "            return img[start_y:start_y+h, start_x:start_x+w]\n",
        "\n",
        "        blurs = [crop_center(img, h, w) for img in blurs]\n",
        "        gt = crop_center(gt, h, w)\n",
        "\n",
        "        # Stack 3 frames into 9-channel input\n",
        "        blur_tensor = torch.cat([self.transform(img) for img in blurs], dim=0)  # Shape: [9, H, W]\n",
        "        gt_tensor = self.transform(gt)  # Shape: [3, H, W]\n",
        "\n",
        "        return blur_tensor.float(), gt_tensor.float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6KpdnHYyojki"
      },
      "outputs": [],
      "source": [
        "val_blur = '/content/gopro_dataset/GoPro/val/blur'\n",
        "val_gt = '/content/gopro_dataset/GoPro/val/GT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M8RhlsVAqhBM"
      },
      "outputs": [],
      "source": [
        "val_dataset = VideoTripletDataset(val_blur, val_gt)  # Replace with your dataset initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ufFL-vcCqmbi",
        "outputId": "be7e0dcb-a590-4b45-ae63-89769ce4807b"
      },
      "outputs": [],
      "source": [
        "model = SimpleTemporalUNet()  # or however you initialize your model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model_checkpoints/best_model.pth', map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "klw1lxOiqs4r",
        "outputId": "0429ab6e-b0b7-4780-909b-87a22e38d5c9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "def visualize_deblur(model, val_dataset, device='cuda'):\n",
        "    input_tensor, target_tensor = val_dataset[101]  # (C=9, H, W) or (C=9, T=3, H, W) depending on your data\n",
        "\n",
        "    # If input tensor shape is (9, H, W), split into 3 frames (3, H, W)\n",
        "    # Extract middle frame (channels 3:6)\n",
        "    input_frame = input_tensor[3:6, :, :]  # middle frame RGB\n",
        "\n",
        "    # Model expects full input (all 9 channels), so prepare batch accordingly\n",
        "    input_batch = input_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_batch)\n",
        "\n",
        "    # Output is probably 3 channels (deblurred RGB)\n",
        "    output_img = output.squeeze(0).cpu().clamp(0,1)\n",
        "    target_img = target_tensor.cpu().clamp(0,1)\n",
        "\n",
        "    # Convert to PIL images\n",
        "    input_img = to_pil_image(input_frame)\n",
        "    output_img = to_pil_image(output_img)\n",
        "    target_img = to_pil_image(target_img)\n",
        "\n",
        "    # Plot images\n",
        "    fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
        "    axs[0].imshow(input_img)\n",
        "    axs[0].set_title(\"Blurred Input (Middle Frame)\")\n",
        "    axs[0].axis('off')\n",
        "\n",
        "    axs[1].imshow(output_img)\n",
        "    axs[1].set_title(\"Deblurred Output\")\n",
        "    axs[1].axis('off')\n",
        "\n",
        "    axs[2].imshow(target_img)\n",
        "    axs[2].set_title(\"Ground Truth\")\n",
        "    axs[2].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "visualize_deblur(model, val_dataset, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
