{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr0i7UZkjRfB",
        "outputId": "1b37ffb8-6b89-49f6-d052-84710f5a8963"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cem5N8-MnnVt",
        "outputId": "95ab2bea-0838-45e2-ef3a-754a133931d0"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "gopro_zip_path = '/content/drive/MyDrive/GoPro.zip'  # replace path of the dataset\n",
        "gopro_extract_path = '/content/gopro_dataset/'\n",
        "\n",
        "# Unzipping GoPro\n",
        "with zipfile.ZipFile(gopro_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(gopro_extract_path)\n",
        "\n",
        "print(\" GoPro dataset unzipped to:\", gopro_extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS-TDxAhnpIk",
        "outputId": "2359e181-88a8-4d82-ca69-f6e56b44e41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moved 2 scenes to validation set.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "def split_train_val(data_dir, val_ratio=0.1, seed=42):\n",
        "    blur_dir = os.path.join(data_dir, 'train', 'blur')\n",
        "    gt_dir = os.path.join(data_dir, 'train', 'GT')\n",
        "    val_blur_dir = os.path.join(data_dir, 'val', 'blur')\n",
        "    val_gt_dir = os.path.join(data_dir, 'val', 'GT')\n",
        "\n",
        "    os.makedirs(val_blur_dir, exist_ok=True)\n",
        "    os.makedirs(val_gt_dir, exist_ok=True)\n",
        "\n",
        "    scenes = sorted(os.listdir(blur_dir))\n",
        "    random.seed(seed)\n",
        "    val_scenes = random.sample(scenes, int(len(scenes) * val_ratio))\n",
        "\n",
        "    for scene in val_scenes:\n",
        "        shutil.move(os.path.join(blur_dir, scene), os.path.join(val_blur_dir, scene))\n",
        "        shutil.move(os.path.join(gt_dir, scene), os.path.join(val_gt_dir, scene))\n",
        "\n",
        "    print(f\"Moved {len(val_scenes)} scenes to validation set.\")\n",
        "\n",
        "# Example usage\n",
        "split_train_val('/content/gopro_dataset/GoPro', val_ratio=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP4tIdA3nsXh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleTemporalUNet(nn.Module):\n",
        "    def __init__(self, in_channels=9, out_channels=3, base_filters=32):\n",
        "        super(SimpleTemporalUNet, self).__init__()\n",
        "\n",
        "        def conv_block(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "        # Encoder path\n",
        "        self.enc1 = conv_block(in_channels, base_filters)       # 9 -> 32\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc2 = conv_block(base_filters, base_filters*2)    # 32 -> 64\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc3 = conv_block(base_filters*2, base_filters*4)  # 64 -> 128\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.enc4 = conv_block(base_filters*4, base_filters*8)  # 128 -> 256\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = conv_block(base_filters*8, base_filters*16)  # 256 -> 512\n",
        "\n",
        "        # Decoder path\n",
        "        self.up4 = nn.ConvTranspose2d(base_filters*16, base_filters*8, kernel_size=2, stride=2)\n",
        "        self.dec4 = conv_block(base_filters*16, base_filters*8)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(base_filters*8, base_filters*4, kernel_size=2, stride=2)\n",
        "        self.dec3 = conv_block(base_filters*8, base_filters*4)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(base_filters*4, base_filters*2, kernel_size=2, stride=2)\n",
        "        self.dec2 = conv_block(base_filters*4, base_filters*2)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(base_filters*2, base_filters, kernel_size=2, stride=2)\n",
        "        self.dec1 = conv_block(base_filters*2, base_filters)\n",
        "\n",
        "        # Final conv (to RGB output)\n",
        "        self.final_conv = nn.Conv2d(base_filters, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)        # [B, 32, H, W]\n",
        "        p1 = self.pool1(e1)      # Downsample\n",
        "\n",
        "        e2 = self.enc2(p1)       # [B, 64, H/2, W/2]\n",
        "        p2 = self.pool2(e2)\n",
        "\n",
        "        e3 = self.enc3(p2)       # [B, 128, H/4, W/4]\n",
        "        p3 = self.pool3(e3)\n",
        "\n",
        "        e4 = self.enc4(p3)       # [B, 256, H/8, W/8]\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(F.max_pool2d(e4, 2))  # [B, 512, H/16, W/16]\n",
        "\n",
        "        # Decoder\n",
        "        d4 = self.up4(b)                               # Upsample\n",
        "        d4 = torch.cat([d4, e4], dim=1)               # Skip connection\n",
        "        d4 = self.dec4(d4)\n",
        "\n",
        "        d3 = self.up3(d4)\n",
        "        d3 = torch.cat([d3, e3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = torch.cat([d2, e2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        out = self.final_conv(d1)\n",
        "        out = torch.sigmoid(out)  # Normalize output to [0,1]\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7NZTUwUnxBX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class VideoTripletDataset(Dataset):\n",
        "    def __init__(self, blur_dir, gt_dir):\n",
        "        self.blur_dir = blur_dir\n",
        "        self.gt_dir = gt_dir\n",
        "        self.samples = []\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "        for scene in sorted(os.listdir(blur_dir)):\n",
        "            blur_scene_path = os.path.join(blur_dir, scene)\n",
        "            gt_scene_path = os.path.join(gt_dir, scene)\n",
        "\n",
        "            if not os.path.isdir(blur_scene_path) or not os.path.isdir(gt_scene_path):\n",
        "                continue\n",
        "\n",
        "            blur_images = sorted(glob.glob(os.path.join(blur_scene_path, \"*.png\")))\n",
        "\n",
        "            for i in range(1, len(blur_images) - 1):\n",
        "                center_name = os.path.basename(blur_images[i])\n",
        "                gt_path = os.path.join(gt_scene_path, center_name)\n",
        "\n",
        "                if os.path.exists(gt_path):\n",
        "                    triplet = [blur_images[i-1], blur_images[i], blur_images[i+1]]\n",
        "                    self.samples.append((triplet, gt_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _read_rgb_image(self, path):\n",
        "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            raise IOError(f\"Failed to load image: {path}\")\n",
        "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        blur_paths, gt_path = self.samples[idx]\n",
        "\n",
        "        # Read triplet + GT\n",
        "        blurs = [self._read_rgb_image(p) for p in blur_paths]\n",
        "        gt = self._read_rgb_image(gt_path)\n",
        "\n",
        "        # Minimal consistent center crop to align sizes\n",
        "        h = min([img.shape[0] for img in blurs] + [gt.shape[0]])\n",
        "        w = min([img.shape[1] for img in blurs] + [gt.shape[1]])\n",
        "        h -= h % 16  # Optional: ensure divisible by 16 for UNet\n",
        "        w -= w % 16\n",
        "\n",
        "        def crop_center(img, h, w):\n",
        "            ch, cw = img.shape[:2]\n",
        "            start_y = (ch - h) // 2\n",
        "            start_x = (cw - w) // 2\n",
        "            return img[start_y:start_y+h, start_x:start_x+w]\n",
        "\n",
        "        blurs = [crop_center(img, h, w) for img in blurs]\n",
        "        gt = crop_center(gt, h, w)\n",
        "\n",
        "        # Stack 3 frames into 9-channel input\n",
        "        blur_tensor = torch.cat([self.transform(img) for img in blurs], dim=0)  # Shape: [9, H, W]\n",
        "        gt_tensor = self.transform(gt)  # Shape: [3, H, W]\n",
        "\n",
        "        return blur_tensor.float(), gt_tensor.float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zucmOKEon0lM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_loaders(train_blur, train_gt, val_blur, val_gt, batch_size=4, num_workers=2):\n",
        "    \"\"\"\n",
        "    Returns PyTorch DataLoaders for training and validation datasets.\n",
        "\n",
        "    Args:\n",
        "        train_blur (str): Path to training blurred frames.\n",
        "        train_gt (str): Path to training ground truth frames.\n",
        "        val_blur (str): Path to validation blurred frames.\n",
        "        val_gt (str): Path to validation ground truth frames.\n",
        "        batch_size (int): Batch size for training loader.\n",
        "        num_workers (int): Number of subprocesses for data loading.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_loader, val_loader)\n",
        "    \"\"\"\n",
        "    train_dataset = VideoTripletDataset(train_blur, train_gt)\n",
        "    val_dataset = VideoTripletDataset(val_blur, val_gt)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True  # Ensures batch size consistency for training\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmB8fAiYn2Si"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4, device='cuda',\n",
        "                drive_path='/content/drive/MyDrive/model_checkpoints', model_name='best_model.pth',\n",
        "                use_amp=True):\n",
        "\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "    best_model_path = os.path.join(drive_path, model_name)\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = -1\n",
        "\n",
        "    # Resume training from saved model\n",
        "    if os.path.exists(best_model_path):\n",
        "        print(f\"Loading best model from {best_model_path}...\")\n",
        "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "        model.eval()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for inputs, targets in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{num_epochs}] Training\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.autocast(device_type=device, enabled=use_amp):\n",
        "                outputs = model(inputs)\n",
        "                loss = F.mse_loss(outputs, targets)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} - Avg Train Loss: {avg_train_loss:.6f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                with torch.autocast(device_type=device, enabled=use_amp):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = F.mse_loss(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"Epoch {epoch+1} - Avg Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Best model saved at epoch {best_epoch} with val loss: {best_val_loss:.6f}\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Training complete. Best epoch: {best_epoch} with val loss: {best_val_loss:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fo6Gm8c3n3_4",
        "outputId": "0432808d-093a-4723-9c20-ed9846341b0c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Dataset paths\n",
        "train_blur = '/content/gopro_dataset/GoPro/train/blur'\n",
        "train_gt = '/content/gopro_dataset/GoPro/train/GT'\n",
        "val_blur = '/content/gopro_dataset/GoPro/val/blur'\n",
        "val_gt = '/content/gopro_dataset/GoPro/val/GT'\n",
        "\n",
        "# Training settings\n",
        "batch_size = 4\n",
        "num_epochs = 60\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "checkpoint_path = '/content/drive/MyDrive/model_checkpoints'\n",
        "model_name = 'best_model.pth'\n",
        "\n",
        "# Load data\n",
        "train_loader, val_loader = get_loaders(\n",
        "    train_blur, train_gt,\n",
        "    val_blur, val_gt,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# Initialize and train model\n",
        "model = SimpleTemporalUNet()\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_epochs=num_epochs,\n",
        "    lr=learning_rate,\n",
        "    device=device,\n",
        "    drive_path=checkpoint_path,\n",
        "    model_name=model_name,\n",
        "    use_amp=True  # Optional: toggle mixed precision training\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
